{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "n1hDDvwZDJqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "id": "cLWcjqM6c_-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n"
      ],
      "metadata": {
        "id": "kKi3TYuVC0JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U\n"
      ],
      "metadata": {
        "id": "kSF5m9VW0IY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "zEV-eO0LHGt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_conversations(input_file, output_file):\n",
        "    requirements = []\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('\",'):\n",
        "            req_list = re.findall(r\"'([^']*)'\", line)\n",
        "            requirements.extend(req_list)\n",
        "\n",
        "    with open(output_file, 'w') as f_output:\n",
        "        for req in requirements:\n",
        "            processed_req = re.sub(r'^\\d+\\.\\s*', '', req.strip())\n",
        "            f_output.write(processed_req + '\\n')\n",
        "\n",
        "    print(f\"Il file è stato elaborato e salvato come {output_file}\")\n",
        "\n",
        "input_file = 'conversazioneRequisiti.txt'\n",
        "output_file = 'outputRequisiti.txt'\n",
        "\n",
        "# Esegui la pulizia delle conversazioni\n",
        "clean_conversations(input_file, output_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbpkxEpQ8HXM",
        "outputId": "2721a74d-e485-4fff-9d18-4a2980421d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il file è stato elaborato e salvato come output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#caricare la lista di requisiti per analizzare le parti del discorso\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def analyze_sentences_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for line_number, sentence in enumerate(lines, start=1):\n",
        "        print(f\"\\nAnalyzing sentence {line_number}: '{sentence.strip()}'\")\n",
        "        doc = nlp(sentence.strip())\n",
        "\n",
        "        print(\"POS tagging and Dependency Parsing:\")\n",
        "        for token in doc:\n",
        "            print(token.text, \"-->\", token.dep_, \"-->\", token.pos_)\n",
        "\n",
        "file_path = 'outputRequisiti.txt'\n",
        "analyze_sentences_from_file(file_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIZvxGR5M8y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "import inflect\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "p = inflect.engine()\n",
        "\n",
        "def to_singular(word):\n",
        "    singular = p.singular_noun(word)\n",
        "    return singular if singular else word\n",
        "\n",
        "def identify_entities_from_text(file_path, output_file):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    identified_entities = set()\n",
        "\n",
        "    for token in doc:\n",
        "       if (token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\") and token.dep_ in [\"nsubj\"]:\n",
        "            entity = to_singular(token.text)\n",
        "            identified_entities.add(entity.lower())\n",
        "\n",
        "    with open(output_file, 'w') as out_file:\n",
        "        for entity in identified_entities:\n",
        "            out_file.write(entity + '\\n')\n",
        "\n",
        "# Esempio di utilizzo\n",
        "file_path = 'outputRequisiti.txt'\n",
        "output_file = 'outputWho.txt'\n",
        "identify_entities_from_text(file_path, output_file)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TjIhmnVaY1vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import pipeline\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_user_story_what(who, requirement):\n",
        "    domanda = f\"Given the requirement {requirement}, what does the {who} aim to do? If there is not the {who} or the aim, answer 'No what was identified in this requirement'\"\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "    answer = qa_pipeline(question=domanda, context=requirement)\n",
        "\n",
        "    user_story_what = answer['answer']\n",
        "\n",
        "    return user_story_what\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_user_story_what_from_file(who, file_path, output_file):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        requirements = file.readlines()\n",
        "\n",
        "    with open(output_file, \"w\") as out_file:\n",
        "        for req in requirements:\n",
        "            req = req.strip()\n",
        "            user_story_what = extract_user_story_what(who, req)\n",
        "            out_file.write(f\"{user_story_what}\\n\")\n",
        "\n",
        "who = [\"user\"]\n",
        "input_file_path = \"outputRequisiti.txt\"\n",
        "output_file_path = \"outputWhat.txt\"\n",
        "extract_user_story_what_from_file(who, input_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "MwENWJdTa-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "def read_excel(file_path):\n",
        "    workbook = openpyxl.load_workbook(file_path)\n",
        "\n",
        "    column_values = []\n",
        "\n",
        "    for sheet_name in workbook.sheetnames:\n",
        "        sheet = workbook[sheet_name]\n",
        "        for row in sheet.iter_rows(min_row=2, max_col=6, values_only=True):\n",
        "            if row[2]:\n",
        "                column_values.append(row[2])\n",
        "\n",
        "            if row[3]:\n",
        "                column_values.append(row[3])\n",
        "\n",
        "    workbook.close()\n",
        "\n",
        "    return column_values\n",
        "\n",
        "def write_to_file(values, output_file):\n",
        "    with open(output_file, 'w') as file:\n",
        "        for value in values:\n",
        "            file.write(f\"{value}\\n\")\n",
        "\n",
        "file_path = \"WhoWhatFinale.xlsx\"\n",
        "output_file = \"outputMetriche.txt\"\n",
        "cell_values = read_excel(file_path)\n",
        "\n",
        "write_to_file(cell_values, output_file)\n"
      ],
      "metadata": {
        "id": "6fT2gpa3B113"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_rouge(reference, prediction):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, prediction)\n",
        "    return scores\n",
        "\n",
        "def main(input_file):\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    references = lines[::2]\n",
        "    predictions = lines[1::2]\n",
        "\n",
        "    rouge1_scores = []\n",
        "    rougeL_scores = []\n",
        "    bleu_scores = []\n",
        "\n",
        "    smoother = SmoothingFunction()\n",
        "\n",
        "    for reference, prediction in zip(references, predictions):\n",
        "        reference = reference.strip()\n",
        "        prediction = prediction.strip()\n",
        "\n",
        "        rouge_scores = calculate_rouge(reference, prediction)\n",
        "        rouge1_score = rouge_scores['rouge1'].fmeasure\n",
        "        rougeL_score = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "        rouge1_scores.append(rouge1_score)\n",
        "        rougeL_scores.append(rougeL_score)\n",
        "\n",
        "        reference_tokens = nltk.word_tokenize(reference)\n",
        "        prediction_tokens = nltk.word_tokenize(prediction)\n",
        "\n",
        "        # Calcolo del punteggio BLEU con smoothing\n",
        "        bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=smoother.method1)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "        print(f\"Reference: {reference}\")\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "        print(f\"ROUGE-1 F-measure: {rouge1_score}\")\n",
        "        print(f\"ROUGE-L F-measure: {rougeL_score}\")\n",
        "        print(f\"BLEU score: {bleu_score}\")\n",
        "        print()\n",
        "\n",
        "    rouge1_avg = sum(rouge1_scores) / len(rouge1_scores)\n",
        "    rougeL_avg = sum(rougeL_scores) / len(rougeL_scores)\n",
        "    bleu_avg = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    print(f\"Overall ROUGE-1 F-measure: {rouge1_avg}\")\n",
        "    print(f\"Overall ROUGE-L F-measure: {rougeL_avg}\")\n",
        "    print(f\"Overall BLEU score: {bleu_avg}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"q3.txt\"\n",
        "    main(input_file)\n"
      ],
      "metadata": {
        "id": "p3_9yGx7mR1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#estrazione del who e what\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "import inflect\n",
        "from transformers import pipeline\n",
        "\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "p = inflect.engine()\n",
        "\n",
        "def to_singular(word):\n",
        "    singular = p.singular_noun(word)\n",
        "    return singular if singular else word\n",
        "\n",
        "def identify_entities_from_text(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    doc = nlp_spacy(text)\n",
        "    identified_entities = {}\n",
        "\n",
        "    for token in doc:\n",
        "        if (token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\") and token.dep_ in [\"nsubj\"]:\n",
        "            token_text_lower = token.text.lower()\n",
        "            if all(entity.lower() != token_text_lower for entity in identified_entities):\n",
        "                entity = to_singular(token.text)\n",
        "                identified_entities[entity.lower()] = []\n",
        "\n",
        "    return identified_entities\n",
        "\n",
        "def extract_user_story_what_from_file(file_path, output_file):\n",
        "    who = identify_entities_from_text(file_path)\n",
        "    print(who)\n",
        "\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        requirements = file.readlines()\n",
        "\n",
        "    for req in requirements:\n",
        "        req = req.strip()\n",
        "        for user in who:\n",
        "            domanda = f\"Given the requirement {req}, what does the {user} aim to do? If there is not the {user} or the aim, answer 'None'\"\n",
        "            answer = qa_pipeline(question=domanda, context=req)\n",
        "            user_story_what = answer['answer']\n",
        "            who[user].append(user_story_what)\n",
        "\n",
        "    with open(output_file, \"w\") as out_file:\n",
        "        for entity, requirements in who.items():\n",
        "            out_file.write(f\"{entity}:\\n\")\n",
        "            for req in requirements:\n",
        "                out_file.write(f\"{req}\\n\")\n",
        "            out_file.write(\"\\n\")\n",
        "\n",
        "file_path = 'requisitiFinali.txt'\n",
        "output_file = 'output_what.txt'\n",
        "\n",
        "extract_user_story_what_from_file(file_path, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx2SazPhPnN_",
        "outputId": "90d9e7e9-4a07-4858-dff4-a1b8dda312c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'system': [], 'user': [], 'team': [], 'ifa': [], 'guest': [], 'administrator': [], 'referee': [], 'fan': [], 'player': [], 'manager': [], 'owner': []}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    }
  ]
}